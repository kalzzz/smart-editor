# 文本去重匹配程序使用说明

这是一个高效的文本去重匹配程序，用于从冗余的语音识别结果中提取与目标文本匹配的词语段。该程序支持中英文混合文本处理，具有相似度匹配和多种匹配算法。

## 功能特点

- **智能匹配算法**：支持动态规划和贪心两种匹配算法
- **相似度匹配**：可以匹配相似但不完全相同的词语
- **中英文混合**：完美支持中文和英文混合文本
- **标点符号处理**：自动忽略标点符号和空格
- **高性能**：优化的算法确保快速处理
- **批量处理**：支持批量处理多个文本

## 文件结构

```
text_deduplication/
├── readme.md          # 原始需求文档
├── text_matcher.py    # 主程序文件
├── example.py         # 使用示例
├── 使用说明.md        # 本文档
└── README.md          # 英文说明文档
```

## 快速开始

### 基本使用

```python
from text_matcher import match_and_filter

# 冗余的语音识别结果
redundant_data = [
    {"conf": 1.0, "end": 0.69, "start": 0.33, "word": "大家好"},
    {"conf": 1.0, "end": 0.9, "start": 0.69, "word": "欢迎"},
    {"conf": 1.0, "end": 1.23, "start": 0.9, "word": "欢"}
]

# 目标文本
target_text = "大家好欢迎"

# 执行匹配
result = match_and_filter(redundant_data, target_text)
print(result)
```

### 输入格式

冗余文本JSON格式：
```json
[
  {
    "conf": 1.0,        // 置信度 (0-1)
    "start": 0.33,      // 起始时间（秒）
    "end": 0.69,        // 结束时间（秒）
    "word": "大家好"     // 词语内容
  }
]
```

### 输出格式

过滤后的JSON格式（保持原有结构）：
```json
[
  {
    "conf": 1.0,
    "start": 0.33,
    "end": 0.69,
    "word": "大家好"
  }
]
```

## API 文档

### match_and_filter()

主要的匹配和过滤函数。

**参数：**
- `redundant_json` (List[Dict]): 冗余文本的JSON格式数据
- `target_text` (str): 目标文本字符串
- `use_dp` (bool, 可选): 是否使用动态规划算法，默认True
- `similarity_threshold` (float, 可选): 相似度匹配阈值，默认0.8

**返回值：**
- `List[Dict]`: 过滤后的词语段列表

### batch_process()

批量处理多个文本匹配任务。

**参数：**
- `data_list` (List[Tuple]): 包含(redundant_json, target_text)元组的列表
- `**kwargs`: 传递给match_and_filter的其他参数

**返回值：**
- `List[List[Dict]]`: 批量处理结果列表

## 算法说明

### 动态规划算法 (use_dp=True)

- **优点**：能找到全局最优解，支持最右匹配优先
- **缺点**：计算复杂度较高
- **适用场景**：对准确性要求高的场景

### 贪心算法 (use_dp=False)

- **优点**：计算速度快，内存占用少
- **缺点**：可能无法找到全局最优解
- **适用场景**：对性能要求高的场景

## 使用示例

### 1. 基本匹配

```python
redundant_data = [
    {"conf": 1.0, "end": 0.69, "start": 0.33, "word": "大家好"},
    {"conf": 1.0, "end": 0.9, "start": 0.69, "word": "欢迎"}
]
target_text = "大家好欢迎"
result = match_and_filter(redundant_data, target_text)
```

### 2. 相似度匹配

```python
redundant_data = [
    {"conf": 0.9, "end": 1.0, "start": 0.0, "word": "机器学习"},
    {"conf": 0.7, "end": 2.0, "start": 1.0, "word": "机械学习"}  # 识别错误
]
target_text = "机器学习"
# 使用较低的相似度阈值来匹配识别错误的词语
result = match_and_filter(redundant_data, target_text, similarity_threshold=0.6)
```

### 3. 中英文混合

```python
redundant_data = [
    {"conf": 1.0, "end": 1.0, "start": 0.0, "word": "Hello"},
    {"conf": 0.85, "end": 4.0, "start": 3.0, "word": "世界"}
]
target_text = "Hello世界"
result = match_and_filter(redundant_data, target_text)
```

### 4. 批量处理

```python
data_sets = [
    (redundant_data_1, "目标文本1"),
    (redundant_data_2, "目标文本2")
]
results = batch_process(data_sets)
```

## 运行测试

运行主程序测试：
```bash
python text_matcher.py
```

运行使用示例：
```bash
python example.py
```

## 性能特点

- **词语级别匹配**：支持中文词语和英文单词的精确匹配
- **标点符号忽略**：自动处理中英文标点符号
- **相似度计算**：使用SequenceMatcher进行高效相似度计算
- **内存优化**：动态规划算法经过内存优化
- **批量处理**：支持高效的批量文本处理

## 注意事项

1. **编码格式**：确保所有文本文件使用UTF-8编码
2. **Python版本**：建议使用Python 3.6+
3. **依赖库**：程序只使用Python标准库，无需额外安装依赖
4. **内存使用**：处理大量数据时，建议使用贪心算法以节省内存
5. **相似度阈值**：根据实际需求调整相似度阈值，过低可能产生误匹配

## 扩展功能

程序设计为可扩展的架构，可以轻松添加：

- 自定义相似度计算方法
- 新的匹配算法
- 特定领域的文本预处理
- 性能监控和日志记录

## 技术实现细节

### 核心算法

1. **文本预处理**：移除标点符号和空格，统一处理中英文文本
2. **动态规划匹配**：使用二维DP表找到最优匹配序列
3. **相似度计算**：基于SequenceMatcher的字符串相似度算法
4. **最右匹配优先**：在多种匹配方案中选择最靠右的匹配

### 时间复杂度

- **动态规划算法**：O(n×m)，其中n为词语数量，m为目标文本长度
- **贪心算法**：O(n×k)，其中k为平均词语长度

### 空间复杂度

- **动态规划算法**：O(n×m)
- **贪心算法**：O(1)

## 许可证

本程序遵循MIT许可证。

---

**开发者**：AI Assistant  
**版本**：1.0  
**最后更新**：2024年